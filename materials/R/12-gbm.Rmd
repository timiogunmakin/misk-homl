---
title: "Gradient Boosting Machines"
output: html_notebook
---

# Prereqs 

```{r slide-3}
# Helper packages
library(tidyverse)

# Modeling packages
library(tidymodels)

# split data
set.seed(123)
ames <- AmesHousing::make_ames()
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

# Basic GBM 

## Default model

```{r}
# create model recipe with all features
lvls <- c("Very_Poor", "Poor", "Fair", "Below_Average", "Average", "Typical", 
          "Above_Average", "Good", "Very_Good", "Excellent", "Very_Excellent")

model_recipe <- recipe(
    Sale_Price ~ ., 
    data = ames_train
  ) %>%
  step_string2factor(
    ends_with("Qual"), ends_with("QC"), ends_with("_Cond"),
    levels = lvls
    ) %>% 
  step_unknown(
    ends_with("Qual"), ends_with("QC"), ends_with("_Cond"), 
    new_level = "None"
    ) %>%
  step_relevel(
    ends_with("Qual"), ends_with("QC"), ends_with("_Cond"), 
    ref_level = "None"
    ) %>%
  step_integer(ends_with("Qual"), ends_with("QC"), ends_with("_Cond")) %>%
  step_dummy(all_nominal(), one_hot = TRUE)
```

```{r}
# create XGBost model object
xgb_mod <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

# create resampling procedure
set.seed(13)
kfold <- vfold_cv(ames_train, v = 5)

# train model
results <- fit_resamples(xgb_mod, model_recipe, kfold)

# model results
collect_metrics(results)
```

## Tuning

```{r}
# create model object
xgb_mod <- boost_tree(
  trees = tune(),
  learn_rate = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

# create grid search design
hyper_grid <- parameters(xgb_mod) %>%
  grid_latin_hypercube(size = 20)
```

```{r}
# perform grid search
results <- tune_grid(xgb_mod, model_recipe, resamples = kfold, grid = hyper_grid)

# model results
show_best(results, metric = "rmse")
```


# Stochastic GBMs

```{r}
# create model object
xgb_mod <- boost_tree(
  trees = 81,
  learn_rate = 0.0683,
  tree_depth = 9,
  min_n = 23,
  sample_size = tune(),
  mtry = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

# create grid search design and search
# across 10 combinations
hyper_grid <- parameters(xgb_mod) %>%
  update(mtry = finalize(mtry(), ames_train)) %>%
  grid_latin_hypercube(size = 20)
```

```{r}
# perform grid search
results <- tune_grid(xgb_mod, model_recipe, resamples = kfold, grid = hyper_grid)

# model results
show_best(results, metric = "rmse")
```

# Regularized GBMs

```{r}
# create model object
xgb_mod <- boost_tree(
  trees = 81,
  learn_rate = 0.0683,
  tree_depth = 9,
  min_n = 23,
  sample_size = 0.828,
  mtry = 66,
  loss_reduction = tune()
) %>% 
  set_engine(
    "xgboost"
    ) %>% 
  set_mode("regression")

# create grid of 10 loss_reducton values
hyper_grid <- parameters(xgb_mod) %>%
  grid_regular(levels = 10)

# perform grid search
results <- tune_grid(xgb_mod, model_recipe, resamples = kfold, grid = hyper_grid)

# model results
show_best(results, metric = "rmse")
```

# Tuning strategy

```{r}
# create model object
xgb_mod <- boost_tree(
  trees = 5000,
  learn_rate = 0.0683,
  tree_depth = 9,
  min_n = 23,
  sample_size = 0.828,
  mtry = 66,
  loss_reduction = 4.64e-3,
  stop_iter = 10
) %>% 
  set_engine(
    "xgboost"
    ) %>% 
  set_mode("regression")

# train model
results <- fit_resamples(xgb_mod, model_recipe, kfold)

# model results
collect_metrics(results)
```

# Feature interpretation

```{r, fig.height=5}
# Create a final fit model 
final_fit <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(xgb_mod) %>%
  fit(data = ames_train)

# plot top 20 influential variables
final_fit %>%
  pull_workflow_fit() %>% 
  vip::vip(num_features = 20)
```


# Exercises

Using the Boston housing data set, where the response feature is the median value of homes within a census tract (`cmedv`):

1. Apply a basic GBM model with the same features you used in the random forest module. 
   - Apply the default hyperparameter settings with a learning rate set to 0.10. How does model performance compare to the random forest module?
   - How many trees were applied? Was this enough to stabilize the loss function or do you need to add more?
   - Tune the tree-based hyperparameters described for basic GBMs. Did your model performance improve?
2. Apply a stochastic GBM model. Tune the hyperparameters using the suggested tuning strategy for stochastic GBMs. Did your model performance improve?
3. Apply regularization to your XGBoost model. Did regularization improve performance?
4. Pick your best GBM model. Which 10 features are considered most influential? Are these the same features that have been influential in previous models?
5. Create partial dependence plots for the top two most influential features. Explain the relationship between the feature and the predicted values.
6. Now perform 1-5 to the Attrition dataset, which is classification model rather than a regression model.

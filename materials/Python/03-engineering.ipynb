{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Feature & Target Engineering\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_density, geom_line, geom_point, ggtitle\n",
    "\n",
    "# Modeling pre-processing with scikit-learn functionality\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling pre-processing with non-scikit-learn packages\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ames housing data\n",
    "ames = pd.read_csv(\"../data/ames.csv\")\n",
    "\n",
    "# create train/test split\n",
    "train, test = train_test_split(ames, train_size=0.7, random_state=123)\n",
    "\n",
    "# separate features from labels and only use numeric features\n",
    "X_train = train.drop(\"Sale_Price\", axis=1)\n",
    "y_train = train[[\"Sale_Price\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TransformedTargetRegressor(transformer=PowerTransformer(method='box-cox'))\n",
    "tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missingness\n",
    "\n",
    "### Visualizing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_raw = pd.read_csv(\"../data/ames_raw.csv\")\n",
    "\n",
    "# count missing values\n",
    "ames_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you identify patterns of missing data?\n",
    "# missingness is represented with white\n",
    "msno.matrix(ames_raw, labels=True, filter=\"bottom\", sort=\"ascending\", n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which features have most missing?\n",
    "# this chart shows the number of observations so small bars (i.e. Pool QC)\n",
    "# represent very few observed values (lots of missingness)\n",
    "msno.bar(ames_raw, labels=True, filter=\"bottom\", sort=\"ascending\", n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "#### Estimated statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median imputation to all features\n",
    "a = SimpleImputer(strategy='median')\n",
    "\n",
    "# median imputation to just numeric predictors\n",
    "b = ColumnTransformer([(\"num_imp\", a, selector(dtype_include=\"number\"))])\n",
    "\n",
    "# median imputation to 1 or more features\n",
    "c = ColumnTransformer([(\"num_imp\", a, selector(\"Gr_Liv_Area\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imp = KNNImputer(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nzv = VarianceThreshold(threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric feature engineering\n",
    "\n",
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing approach\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "# Normalize all numeric features\n",
    "X_norm = ColumnTransformer([(\"norm\", yj, selector(dtype_include=\"number\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing approach\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# standardize all numeric features\n",
    "std = ColumnTransformer([(\"norm\", scaler, selector(dtype_include=\"number\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature engineering\n",
    "\n",
    "### One-hot & dummy encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# apply to all categorical features\n",
    "ohe = ColumnTransformer([(\"one-hot\", encoder, selector(dtype_include=\"object\"))])\n",
    "\n",
    "# dummy encode\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "\n",
    "# apply to all categorical features\n",
    "de = ColumnTransformer([(\"dummy\", encoder, selector(dtype_include=\"object\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Label encode a single column\n",
    "lbl = ColumnTransformer([(\"label\", encoder, \"MS_SubClass\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID all quality features to ordinal encode\n",
    "cols = list(X_train.filter(regex=(\"Qual$|QC$|Cond$\")).columns)\n",
    "\n",
    "# specify levels in order\n",
    "lvs = [\"Very_Poor\", \"Poor\", \"Fair\", \"Below_Average\", \"Average\", \"Typical\", \n",
    "       \"Above_Average\", \"Good\", \"Very_Good\", \"Excellent\", \"Very_Excellent\"]\n",
    "val = range(0, len(lvs))\n",
    "\n",
    "# create a level to integer mapping\n",
    "lvl_map = dict(zip(lvs, val))\n",
    "category_mapping = [{'col': col, 'mapping': lvl_map} for col in cols]\n",
    "\n",
    "# example of first two mappings\n",
    "category_mapping[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ordinal encoder\n",
    "cat_encoder = OrdinalEncoder(cols=cols, mapping=category_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rare label encoder\n",
    "rare_encoder = RareLabelEncoder(tol=0.01, replace_with=\"other\")\n",
    "\n",
    "# demonstrate how some neighborhoods are now represented by \"other\"\n",
    "rare_encoder.fit_transform(X_train)[\"Neighborhood\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA object - keep 25 components\n",
    "pca = PCA(n_components=25)\n",
    "\n",
    "# apply PCA to all numeric features\n",
    "pca_encoder = ColumnTransformer([(\"pca\", pca, selector(dtype_include=\"number\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the process together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test split\n",
    "train, test = train_test_split(ames, train_size=0.7, random_state=123)\n",
    "\n",
    "# separate features from labels and only use numeric features\n",
    "X_train = train.drop(\"Sale_Price\", axis=1)\n",
    "y_train = train[\"Sale_Price\"]\n",
    "\n",
    "# create KNN model object\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# define loss function\n",
    "loss = 'neg_root_mean_squared_error'\n",
    "\n",
    "# create 10 fold CV object\n",
    "kfold = KFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "# Create grid of hyperparameter values\n",
    "hyper_grid = {'knn__n_neighbors': range(2, 26)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove near-zero variance features that are categorical  \n",
    "nzv_encoder = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "# 2. Ordinal encode our quality-based features \n",
    "ord_cols = list(X_train.filter(regex=(\"Qual$|QC$|Cond$\")).columns)\n",
    "lvs = [\"Very_Poor\", \"Poor\", \"Fair\", \"Below_Average\", \"Average\", \"Typical\", \n",
    "       \"Above_Average\", \"Good\", \"Very_Good\", \"Excellent\", \"Very_Excellent\"]\n",
    "val = range(0, len(lvs))\n",
    "lvl_map = dict(zip(lvs, val))\n",
    "category_mapping = [{'col': col, 'mapping': lvl_map} for col in ord_cols]\n",
    "ord_encoder = OrdinalEncoder(cols=ord_cols, mapping=category_mapping)\n",
    "\n",
    "# 3. Center and scale (i.e., standardize) all numeric features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 4. Perform dimension reduction by applying PCA to all numeric features\n",
    "pca = PCA(n_components=30)\n",
    "\n",
    "# 5. One-hot encode remaining categorical features.\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# combine all steps into a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "  remainder=\"passthrough\",\n",
    "  transformers=[\n",
    "  (\"nzv_encode\", nzv_encoder, selector(dtype_include=\"number\")),\n",
    "  (\"ord_encode\", ord_encoder, ord_cols),\n",
    "  (\"std_encode\", scaler, selector(dtype_include=\"number\")),\n",
    "  (\"pca_encode\", pca, selector(dtype_include=\"number\")),\n",
    "  (\"one-hot\", encoder, selector(dtype_include=\"object\")),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "  (\"preprocessor\", preprocessor),\n",
    "  (\"knn\", knn),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune a knn model using grid search\n",
    "grid_search = GridSearchCV(model_pipeline, hyper_grid, cv=kfold, scoring=loss)\n",
    "results = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model's cross validated RMSE\n",
    "abs(results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model's k value\n",
    "results.best_estimator_.get_params().get('knn__n_neighbors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all RMSE results\n",
    "all_rmse = pd.DataFrame({'k': range(2, 26), \n",
    "                         'RMSE': np.abs(results.cv_results_['mean_test_score'])})\n",
    "\n",
    "(ggplot(all_rmse, aes(x='k', y='RMSE'))\n",
    " + geom_line()\n",
    " + geom_point()\n",
    " + ggtitle(\"Cross validated grid search results\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Ames dataset and the same approach shown in the [last section](#engineering-process-example):\n",
    "\n",
    "1. Rather than use a 70% stratified training split, try an 80% unstratified training split. How does your cross-validated results compare?\n",
    "\n",
    "2. Rather than numerically encode the quality and condition features (i.e. `step_integer(matches(\"Qual|Cond|QC\"))`), one-hot encode these features.  What is the difference in the number of features in your training set?  Apply the same cross-validated KNN model to this new feature set. How does the performance change? How does the training time change?\n",
    "\n",
    "3. Identify three new feature engineering steps that are provided by [recipes](https://recipes.tidymodels.org/), [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) or some other open source R/Python package:\n",
    "   - Why would these feature engineering steps be applicable to the Ames data?\n",
    "   - Apply these feature engineering steps along with the same cross-validated KNN model. How do your results change?\n",
    "   \n",
    "4. Using the [Attrition data set](https://misk-data-science.github.io/misk-homl/docs/01-introduction.nb.html#the_data_sets), assess the characteristics of the target and features.\n",
    "   - Which target/feature engineering steps should be applied?\n",
    "   - Create a feature engineering pipeline and apply a KNN grid search. What is the performance of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
title: "Multivariate Adaptive Regression Splines"
output:
  html_notebook:
    toc: yes
    toc_float: true
bibliography: [references.bib, packages.bib]
---

```{r setup, include=FALSE}
# Set global R options
options(scipen = 999)

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE, 
  message = FALSE
)
```

The previous modules discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding nonlinear model terms (e.g., squared terms, interaction effects, and other transformations of the original features); however, to do so you the analyst must know the specific nature of the nonlinearities and interactions _a priori_.  Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities and interactions in the data that help maximize predictive accuracy. 

This module discusses _multivariate adaptive regression splines_\index{multivariate adaptive regression splines} (MARS) [@friedman1991multivariate], an algorithm that automatically creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of multiple linear regression. Future modules will focus on other nonlinear algorithms.


# Prerequisites

For this module we will use the following packages:

```{r 10-pkgs, message=FALSE}
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting

# Modeling packages
library(earth)     # for fitting MARS models
library(caret)     # for automating the tuning process

# Model interpretability packages
library(vip)       # for variable importance
library(pdp)       # for variable relationships
```

To illustrate various concepts we'll continue with the Ames housing data.

```{r MARS-ames-train}
# Stratified sampling with the rsample package
set.seed(123)
ames <- AmesHousing::make_ames()
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

# The basic idea

In the previous modules, we focused on linear models (where the analyst has to explicitly specify any nonlinear relationships and interaction effects). We illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients.  However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictive accuracy.

We can extend linear models to capture any non-linear relationship. Typically, this is done by explicitly including polynomial terms (e.g., $x_i^2$) or step functions.  Polynomial regression is a form of regression in which the relationship between $X$ and $Y$ is modeled as a $d$th degree polynomial in $X$.  For example, Equation \@ref(eq:poly) represents a polynomial regression function where $Y$ is modeled as a $d$-th degree polynomial in $X$.  Generally speaking, it is unusual to use $d$ greater than 3 or 4 as the larger $d$ becomes, the easier the function fit becomes overly flexible and oddly shaped...especially near the boundaries of the range of $X$ values. Increasing $d$ also tends to increase the presence of multicollinearity.

\begin{equation}
  y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i \dots + \beta_d x^d_i + \epsilon_i,
\end{equation}

An alternative to polynomials is to use step functions. Whereas polynomial functions impose a global non-linear relationship, step functions break the range of $X$ into bins, and fit a simple constant (e.g., the mean response) in each. This amounts to converting a continuous feature into an ordered categorical variable such that our linear regression function is converted to the following equation

\begin{equation}
  y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \beta_3 C_3(x_i) \dots + \beta_d C_d(x_i) + \epsilon_i,
\end{equation}

where $C_1(x_i)$ represents $x_i$ values ranging from $c_1 \leq x_i < c_2$, $C_2\left(x_i\right)$ represents $x_i$ values ranging from $c_2 \leq x_i < c_3$, $\dots$, $C_d\left(x_i\right)$ represents $x_i$ values ranging from $c_{d-1} \leq x_i < c_d$.  The figure below contrasts linear, polynomial, and step function fits for non-linear, non-monotonic simulated data.


```{r nonlinear-comparisons, echo=FALSE, fig.height=6, fig.width=8, fig.cap="Blue line represents predicted (`y`) values as a function of `x` for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional linear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function cutting `x` into six categorical levels."}
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 6)

p1 <- ggplot(df, aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("(A) Assumed linear relationship")

p2 <- ggplot(df, aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 2, raw = TRUE)) +
  ggtitle("(B) Degree-2 polynomial regression")

p3 <- ggplot(df, aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 3, raw = TRUE)) +
  ggtitle("(C) Degree-3 polynomial regression")

# fit step function model (6 steps)
step_fit <- lm(y ~ cut(x, 5), data = df)
step_pred <- predict(step_fit, df)

p4 <- ggplot(cbind(df, step_pred), aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = step_pred), size = 1, color = "blue") +
  ggtitle("(D) Step function regression")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Although useful, the typical implementation of polynomial regression and step functions require the user to explicitly identify and incorporate which variables should have what specific degree of interaction or at what points of a variable $X$ should cut points be made for the step functions.  Considering many data sets today can easily contain 50, 100, or more features, this would require an enormous and unnecessary time commitment from an analyst to determine these explicit non-linear settings.

## Multivariate adaptive regression splines

Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints (_knots_) similar to step functions.  The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s).  For example, consider our non-linear, non-monotonic data above where $Y = f\left(X\right)$. The MARS procedure will first look for the single point across the range of `X` values where two different linear relationships between `Y` and `X` achieve the smallest error (e.g., smallest SSE).  What results is known as a hinge function $h\left(x-a\right)$, where $a$ is the cutpoint value. For a single knot (Figure \@ref(fig:examples-of-multiple-knots) (A)), our hinge function is $h\left(\text{x}-1.183606\right)$ such that our two linear models for `Y` are

\begin{equation}
  \text{y} = 
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606
  \end{cases}
\end{equation}

Once the first knot has been found, the search continues for a second knot which is found at $x = 4.898114$ (plot B in figure below).  This results in three linear models for `y`:

\begin{equation}
  \text{y} = 
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606 \quad \& \quad \text{x} < 4.898114, \\
    \beta_0 + \beta_1(4.898114 - \text{x}) & \text{x} > 4.898114
  \end{cases}
\end{equation}

```{r examples-of-multiple-knots, echo=FALSE, fig.height=6, fig.width=8, fig.cap="Examples of fitted regression splines of one (A), two (B), three (C), and four (D) knots."}
# one knot
mars1 <- mda::mars(df$x, df$y, nk = 3, prune = FALSE)
p1 <- df %>%
  mutate(predicted = as.vector(mars1$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(A) One knot")

# two knots
mars2 <- mda::mars(df$x, df$y, nk = 5, prune = FALSE)
p2 <- df %>%
  mutate(predicted = as.vector(mars2$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(B) Two knots")

mars3 <- mda::mars(df$x, df$y, nk = 7, prune = FALSE)
p3 <- df %>%
  mutate(predicted = as.vector(mars3$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(C) Three knots")

mars4 <- mda::mars(df$x, df$y, nk = 9, prune = FALSE)
p4 <- df %>%
  mutate(predicted = as.vector(mars4$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(D) Four knots")


gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

This procedure continues until many knots are found, producing a (potentially) highly non-linear prediction equation.  Although including many knots may allow us to fit a really good relationship with our training data, it may not generalize very well to new, unseen data. Consequently, once the full set of knots has been identified, we can sequentially remove knots that do not contribute significantly to predictive accuracy.  This process is known as "pruning" and we can use cross-validation, as we have with the previous models, to find the optimal number of knots.

# Fitting a basic MARS model

We can fit a direct engine MARS model with the __earth__ package [@R-earth]. By default, `earth::earth()` will assess all potential knots across all supplied features and then will prune to the optimal number of knots based on an expected change in $R^2$ (for the training data) of less than 0.001.  This calculation is performed by the Generalized cross-validation (GCV)  procedure, which is a computational shortcut for linear models that produces an approximate leave-one-out cross-validation error metric [@golub1979generalized]. 

> ___Note:___ _The term "MARS" is trademarked and licensed exclusively to Salford Systems: https://www.salford-systems.com. We can use MARS as an abbreviation; however, it cannot be used for competing software solutions.  This is why the R package uses the name **earth**. Although, according to the package documentation, a backronym for "earth" is "Enhanced Adaptive Regression Through Hinges"._

The following applies a basic MARS model to our __ames__ example. The results show us the final models GCV statistic, generalized $R^2$ (GRSq), and more.  

```{r fit-basic-model}
# Fit a basic MARS model
mars1 <- earth(
  Sale_Price ~ .,  
  data = ames_train   
)

# Print model summary
print(mars1)
```

It also shows us that 36 of 40 terms were used from 28 of the 307 original predictors. But what does this mean?  If we were to look at all the coefficients, we would see that there are 36 terms in our model (including the intercept).  These terms include hinge functions produced from the original 307 predictors (307 predictors because the model automatically dummy encodes categorical features). Looking at the first 10 terms in our model, we see that  `Gr_Liv_Area` is included with a knot at 2787 (the coefficient for $h\left(2787-\text{Gr_Liv_Area}\right)$ is -50.68), `Year_Built` is included with a knot at 2004, etc.

> ___Pro Tip:___ _You can check out all the coefficients with `summary(mars1)` or `coef(mars1)`._

```{r basic-mod-coef}
summary(mars1) %>% .$coefficients %>% head(10)
```

The plot method for MARS model objects provides useful performance and residual plots.  The below figure illustrates the model selection plot that graphs the GCV $R^2$ (left-hand $y$-axis and solid black line) based on the number of terms retained in the model ($x$-axis) which are constructed from a certain number of original predictors (right-hand $y$-axis). The vertical dashed lined at 36 tells us the optimal number of terms retained where marginal increases in GCV $R^2$ are less than 0.001.

```{r basic-mod-plot, fig.width=5, fig.height=3.5, fig.cap="Model summary capturing GCV $R^2$ (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 35 non-intercept terms were retained which are based on 27 predictors.  Any additional terms retained in the model, over and above these 35, result in less than 0.001 improvement in the GCV $R^2$."}
plot(mars1, which = 1)
```

In addition to pruning the number of knots, `earth::earth()` allows us to also assess potential interactions between different hinge functions. The following illustrates this by including a `degree = 2` argument. You can see that now our model includes interaction terms between a maximum of two hinge functions (e.g., `h(Bsmt_Unf_SF-1017)*h(2787-Gr_Liv_Area)` represents an interaction effect for those houses with more than 1,017 unfinished basement square footage but less than 2,787 above ground living space).

```{r fit-basic-model-m2}
# Fit a basic MARS model
mars2 <- earth(
  Sale_Price ~ .,  
  data = ames_train,
  degree = 2
)

# check out the first 10 coefficient terms
summary(mars2) %>% .$coefficients %>% head(10)
```

# Tuning

There are two important tuning parameters associated with our MARS model: the maximum degree of interactions and the number of terms retained in the final model. We need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of CV model performance on the training data rather than an exact _k_-fold CV process). As in previous modules, we'll perform a CV grid search to identify the optimal hyperparameter mix.  Below, we set up a grid that assesses 30 different combinations of interaction complexity (`degree`) and the number of terms to retain in the final model (`nprune`).

> ___Pro Tip:___ _Rarely is there any benefit in assessing greater than 3-rd degree interactions and we suggest starting out with 10 evenly spaced values for `nprune` and then you can always zoom in to a region once you find an approximate optimal solution._

```{r tuning-grid}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)

head(hyper_grid)
```

As in the previous modules, we can use __caret__ to perform a grid search using 10-fold CV.  The model that provides the optimal combination includes second degree interaction effects and retains 56 terms. 

> ___Warning:___ _This grid search took roughly five minutes to complete._

```{r grid-search, fig.cap="Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains 56 terms and includes up to 2$^{nd}$ degree interactions.", fig.height=3}
# Cross-validated model
set.seed(123)  # for reproducibility
cv_mars <- train(
  x = subset(ames_train, select = -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# View results
cv_mars$bestTune

cv_mars$results %>%
  filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)
```

The cross-validated RMSE for these models is displayed below; the optimal model's cross-validated RMSE was \$27,8999.

```{r}
ggplot(cv_mars)
```

The above grid search helps to focus where we can further refine our model tuning. As a next step, we could perform a grid search that focuses in on a refined grid space for `nprune` (e.g., comparing 45--65 terms retained). However, for brevity we'll leave this as an exercise for the reader.

So how does this compare to our previously built models for the Ames housing data?  The following table compares the cross-validated RMSE for our tuned MARS model to an ordinary multiple regression model along with tuned principal component regression (PCR), partial least squares (PLS), and regularized regression (elastic net) models.  

> ___Note:___ _Notice that our elastic net model is higher than in the last chapter. This table compares these 5 modeling approaches without performing any logarithmic transformation on the target variable._

```{r cv-model-comparison, echo=FALSE, fig.cap="Cross-validated RMSE results for tuned MARS and regression models.", warning=FALSE, message=FALSE}
set.seed(123)
cv_model1 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10)
)

set.seed(123)
cv_model2 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
)

set.seed(123)
cv_model3 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
)

set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train,
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  tuneLength = 10
)

# extract out of sample performance measures
summary(resamples(list(
  "LM" = cv_model1, 
  "PCR" = cv_model2, 
  "PLS" = cv_model3,
  "ENET" = cv_model4,
  "MARS" = cv_mars
)))$statistics$RMSE %>%
  kableExtra::kable(caption = "Cross-validated RMSE results for tuned MARS and regression models.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

Although the MARS model did not have a lower mean RMSE than the elastic net and PLS models, you can see that the the median RMSE was actually lower. We can look at the RMSEs across the 10 cross validation iterations. We see that most of them are in the low \$20K range; however, there is one fold (`Fold08`) that had an extremely large RMSE that is skewing the mean RMSE for the MARS model. This would be worth exploring as there are likely some unique observations that are skewing the results.

```{r, warning=FALSE, message=FALSE}
cv_mars$resample
```

# Feature interpretation {#mars-features}

MARS models via `earth::earth()` include a backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. This total reduction is used as the variable importance measure (`value = "gcv"`). Since MARS will automatically include and exclude terms during the pruning process, it essentially performs automated feature selection. If a predictor was never used in any of the MARS basis functions in the final model (after pruning), it has an importance value of zero. This is illustrated in the plot below where 27 features have $>0$ importance values while the rest of the features have an importance value of zero since they were not included in the final model.  Alternatively, you can also monitor the change in the residual sums of squares (RSS) as terms are added (`value = "rss"`); however, you will see very little difference between these methods.

```{r vip, fig.height=5.5, fig.width=9, fig.cap="Variable importance based on impact to GCV (left) and RSS (right) values as predictors are added to the model. Both variable importance measures will usually give you very similar results."}
# variable importance plots
p1 <- vip(cv_mars, num_features = 40, geom = "point", value = "gcv") + ggtitle("GCV")
p2 <- vip(cv_mars, num_features = 40, geom = "point", value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Its important to realize that variable importance will only measure the impact of the prediction error as features are included; however, it does not measure the impact for particular hinge functions created for a given feature.  For example, in the above plot we saw that `Gr_Liv_Area` and `Year_Built` are the two most influential variables; however, variable importance does not tell us how our model is treating the non-linear patterns for each feature.  Also, if we look at the interaction terms our model retained, we see interactions between different hinge functions.

```{r}
# extract coefficients, convert to tidy data frame, and
# filter for interaction terms
cv_mars$finalModel %>%
  coef() %>%  
  broom::tidy() %>%  
  filter(stringr::str_detect(names, "\\*")) 
```

To better understand the relationship between these features and `Sale_Price`, we can create partial dependence plots (PDPs) for each feature individually and also together. The individual PDPs illustrate that our model found knots for each feature provides the best fit.  For example, as homes exceed 2,787 square feet, each additional square foot demands a smaller marginal increase in sale price than homes with less than 2,787 square feet.  Similarly, for homes built in 2004 or later, there is a greater marginal effect on sales price based on the age of the home than for homes built prior to 2004.  The interaction plot (far right figure) illustrates how these features appear to effect the predicted sales price together. 

```{r pdp, fig.width=15, fig.height=3, warning=FALSE, message=FALSE}
# Construct partial dependence plots
p1 <- partial(cv_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  ggplot(aes(Gr_Liv_Area, yhat)) +
  geom_line()

p2 <- partial(cv_mars, pred.var = "Year_Built", grid.resolution = 10) %>% 
  ggplot(aes(Year_Built, yhat)) +
  geom_line()

p3 <- partial(cv_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), 
              grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, 
              screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

# Attrition data

The MARS method and algorithm can be extended to handle classification problems and GLMs in general.^[See @esl and @stone1997polynomial for technical details regarding various alternative encodings for binary and mulinomial classification approaches.] In the [logistic regression module](https://misk-data-science.github.io/misk-homl/docs/05-logistic-regression.nb.html) we saw a slight improvement in our cross-validated accuracy rate using regularized regression.  Here, we tune a MARS model using the same search grid as we did above.  We see our best models include no interaction effects and the optimal model retained 12 terms.

```{r tuned-marts-attrition, fig.cap="Cross-validated accuracy rate for the 30 different hyperparameter combinations in our grid search. The optimal model retains 12 terms and includes no interaction effects."}
# get attrition data
df <- rsample::attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- rsample::initial_split(df, prop = .7, strata = "Attrition")
churn_train <- rsample::training(churn_split)
churn_test  <- rsample::testing(churn_split)


# for reproducibiity
set.seed(123)

# cross validated model
tuned_mars <- train(
  x = subset(churn_train, select = -Attrition),
  y = churn_train$Attrition,
  method = "earth",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# best model
tuned_mars$bestTune

# plot results
ggplot(tuned_mars)
```

However, comparing our MARS model to the previous linear models (logistic regression and regularized regression), we do not see any improvement in our overall accuracy rate.

```{r attrition-modeling-mars, echo=FALSE, fig.cap="Cross-validated accuracy results for tuned MARS and regression models."}
# train logistic regression model
set.seed(123)
glm_mod <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
  )

# train regularized logistic regression model
set.seed(123)
penalized_mod <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
  )

# extract out of sample performance measures
summary(resamples(list(
  Logistic_model = glm_mod, 
  Elastic_net = penalized_mod,
  MARS_model = tuned_mars
  )))$statistics$Accuracy %>%
  kableExtra::kable(caption = "Cross-validated accuracy results for tuned MARS and regression models.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

# Final thoughts

There are several advantages to MARS.  First, MARS naturally handles mixed types of predictors (quantitative and qualitative). MARS considers all possible binary partitions of the categories for a qualitative predictor into two groups.^[This is very similar to CART-like decision trees which you'll be exposed to in a later module.] Each group then generates a pair of piecewise indicator functions for the two categories. MARS also requires minimal feature engineering (e.g., feature scaling) and performs automated feature selection. For example, since MARS scans each predictor to identify a split that improves predictive accuracy, non-informative features will not be chosen. Furthermore, highly correlated predictors do not impede predictive accuracy as much as they do with OLS models. 

However, one disadvantage to MARS models is that they're typically slower to train.  Since the algorithm scans each value of each predictor for potential cutpoints, computational performance can suffer as both $n$ and $p$ increase.  Also, although correlated predictors do not necessarily impede model performance, they can make model interpretation difficult. When two features are nearly perfectly correlated, the algorithm will essentially select the first one it happens to come across when scanning the features.  Then, since it randomly selected one, the correlated feature will likely not be included as it adds no additional explanatory power.  

# Python

The `py-earth` Python package provides an implementation of MARS. You can see the documentation [here](http://mehdidc.github.io/pyearth-doc/#) and several examples in the [examples subdirectory](https://github.com/scikit-learn-contrib/py-earth/tree/master/examples) of the [github repo](https://github.com/scikit-learn-contrib/py-earth).

# Exercises

Using the `Hitters` dataset from the ISLR package (`data(Hitters, package = "ISLR")`):

1. Apply a MARS model with all features.
2. How does the model performance compare to your previous models?
3. How many of the features are influential? Which 10 features are considered most influential?
4. Does your model include hinge functions? If so, explain their coefficient and plot their impact on the predicted response variable.
5. Does your model include interactions? If so, pick the interaction effect that is most influential and explain the coefficient.
6. **Python challenge**: Save the `Hitters` data from R to a CSV file. Import this data into a Python session. Now Repeat the above exercises but using Python and Scikit Learn.

[üè†](https://github.com/misk-data-science/misk-homl)

# References

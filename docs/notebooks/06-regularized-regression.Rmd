---
title: "Regularized Regression"
output:
  html_document:
    toc: yes
    toc_float: true
    css: style.css
bibliography: [references.bib, packages.bib]
---

<br>

```{r setup, include=FALSE}

# Set global knitr chunk options
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      collapse = TRUE, fig.align = 'center')

library(reticulate)
use_virtualenv("/Users/b294776/Desktop/Workspace/Projects/misk/misk-homl/venv", required = TRUE)

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# hidden package requirements
library(glmnet)
```

```{python, echo = FALSE}
import plotnine
plotnine.themes.theme_set(new=plotnine.themes.theme_light())
```

Linear models (LMs) provide a simple, yet effective, approach to predictive modeling. Moreover, when certain assumptions required by LMs are met (e.g., constant variance), the estimated coefficients are unbiased and, of all linear unbiased estimates, have the lowest variance. However, in today‚Äôs world, data sets being analyzed typically contain a large number of features. As the number of features grow, certain assumptions typically break down and these models tend to overfit the training data, causing our out of sample error to increase. __Regularization__ methods provide a means to constrain or _regularize_ the estimated coefficients, which can reduce the variance and decrease out of sample error.

# Learning objectives

By the end of this module you will know how to:

* Apply ridge, lasso, and elastic net regularized models.
* Tune the magnitude and type of the regularization penalty to find an optimal model.
* Extract and visualize the most influential features.

# Prerequisites {.tabset}

## `r fontawesome::fa("python")` 

```{python}
# Helper packages
import numpy as np
import pandas as pd
from plotnine import *

# Modeling packages
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold

# Minimize convergence warning messages
import warnings
warnings.filterwarnings("ignore")
```

```{python}
# Ames housing data
ames = pd.read_csv("data/ames.csv")

# create train/test split
train, test = train_test_split(ames, train_size=0.7, random_state=123)

# separate features from labels and only use numeric features
X_train = train.drop("Sale_Price", axis=1)
y_train = train[["Sale_Price"]]
```


## `r fontawesome::fa("r-project")`

```{r}
# Helper packages
library(tidyverse) # general data munging & visualization

# Modeling packages
library(tidymodels)

# Model interpretability packages
library(vip)      # for variable importance
```

```{r 06-ames-train}
# Stratified sampling with the rsample package
ames <- AmesHousing::make_ames()
set.seed(123)  # for reproducibility
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

# Why regularize?

The easiest way to understand regularized regression is to explain how and why it is applied to ordinary least squares (OLS). The objective in OLS regression is to find the _hyperplane_ (e.g., a straight line in two dimensions) that minimizes the sum of squared errors (SSE) between the observed and predicted response values (see Figure below). This means identifying the hyperplane that minimizes the grey lines, which measure the vertical distance between the observed (red dots) and predicted (blue line) response values.

```{r hyperplane, echo=FALSE, fig.cap="Figure: Fitted regression line using Ordinary Least Squares."}
ames_sub <- ames_train %>%
  filter(Gr_Liv_Area > 1000 & Gr_Liv_Area < 3000) %>%
  sample_frac(.5)
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_sub)

model1 %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = .fitted), 
               alpha = 0.3) +
  geom_point(size = 1, color = "red") +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar)
```

More formally, the objective function being minimized can be written as:

\begin{equation}
\text{minimize} \left( SSE = \sum^n_{i=1} \left(y_i - \hat{y}_i\right)^2 \right)
\end{equation}

As we discussed in the [linear regression module](https://misk-data-science.github.io/misk-homl/docs/notebooks/04-linear-regression.html), the OLS objective function performs quite well when our data adhere to a few key assumptions:

* Linear relationship;
* There are more observations (_n_) than features (_p_) ($n > p$); 
* No or little multicollinearity.

```{block, type="note"}
For classical statistical inference procedures (e.g., confidence intervals based on the classic t-statistic) to be valid, we also need to make stronger assumptions regarding normality (of the errors) and homoscedasticity (i.e., constant error variance).
```

Many real-life data sets, like those common to _text mining_ and _genomic studies_ are _wide_, meaning they contain a larger number of features ($p > n$).  As _p_ increases, we're more likely to violate some of the OLS assumptions and alternative approaches should be considered.  This was briefly illustrated in the [linear regression module](https://misk-data-science.github.io/misk-homl/docs/04-linear-regression.nb.html#model_concerns) where the presence of multicollinearity was diminishing the interpretability of our estimated coefficients due to inflated variance.  By reducing multicollinearity, we were able to increase our model's accuracy. Of course, multicollinearity can also occur when $n > p$. 

Having a large number of features invites additional issues in using classic regression models. For one, having a large number of features makes the model much less interpretable. Additionally, when $p > n$, there are many (in fact infinite) solutions to the OLS problem! In such cases, it is useful (and practical) to assume that a smaller subset of the features exhibit the strongest effects (something called the _bet on sparsity principle_ [see @hastie2015statistical, p. 2].). For this reason, we sometimes prefer estimation techniques that incorporate _feature selection_\index{feature selection}. One approach to this is called _hard thresholding_ feature selection, which includes many of the traditional linear model selection approaches like _forward selection_ and _backward elimination_. These procedures, however, can be computationally inefficient, do not scale well, and treat a feature as either in or out of the model (hence the name hard thresholding). In contrast, a more modern approach, called _soft thresholding_, slowly pushes the effects of irrelevant features toward zero, and in some cases, will zero out entire coefficients. As will be demonstrated, this can result in more accurate models that are also easier to interpret.

With wide data (or data that exhibits multicollinearity), one alternative to OLS regression is to use regularized regression (also commonly referred to as _penalized_ models\index{penalized models} or _shrinkage_ methods\index{shrinkage methods} as in @esl and @apm) to constrain the total size of all the coefficient estimates. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model (at the expense of no longer being unbiased---a reasonable compromise).

The objective function of a regularized regression model is similar to OLS, albeit with a penalty term $P$. 

\begin{equation}
\text{minimize} \left( SSE + P \right)
\end{equation}

This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE).

This concept generalizes to all GLM models (e.g., logistic and Poisson regression) and even some _survival models_. So far, we have been discussing OLS and the sum of squared errors loss function. However, different models within the GLM family have different loss functions (see Chapter 4 of @esl). Yet we can think of the penalty parameter all the same---it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model‚Äôs loss function.

There are three common penalty parameters we can implement:

1. Ridge;
2. Lasso (or LASSO);
3. Elastic net (or ENET), which is a combination of ridge and lasso.

## Ridge penalty {#ridge}

Ridge regression [@hoerl1970ridge] controls the estimated coefficients by adding <font color="red">$\lambda \sum^p_{j=1} \beta_j^2$</font> to the objective function. 

\begin{equation}
\text{minimize } \left( SSE + \lambda \sum^p_{j=1} \beta_j^2 \right)
\end{equation}

The size of this penalty, referred to as $L^2$ (or Euclidean) norm, can take on a wide range of values, which is controlled by the _tuning parameter_ $\lambda$.  When $\lambda = 0$ there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE.  However, as $\lambda \rightarrow \infty$, the penalty becomes large and forces the coefficients toward zero (but not all the way). This is illustrated below where exemplar coefficients have been regularized with $\lambda$ ranging from 0 to over 8,000. 

```{r ridge-coef-example, echo=FALSE, fig.cap="Figure: Ridge regression coefficients for 15 exemplar predictor variables as $\\lambda$ grows from  $0 \\rightarrow \\infty$. As $\\lambda$ grows larger, our coefficient magnitudes are more constrained.", fig.height=3.5, fig.width=7}
boston_train_x <- model.matrix(cmedv ~ ., pdp::boston)[, -1]
boston_train_y <- pdp::boston$cmedv

# model
boston_ridge <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 0
)

lam <- boston_ridge$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_ridge$a0 %>% names()) %>%
  rename(lambda = ".")

results <- boston_ridge$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)

result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))

ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.06, show.legend = FALSE)
```

Although these coefficients were scaled and centered prior to the analysis, you will notice that some are quite large when $\lambda$ is near zero.  Furthermore, you'll notice that feature `x1` has a large negative parameter that fluctuates until $\lambda \approx 7$ where it then continuously shrinks toward zero.  This is indicative of multicollinearity and likely illustrates that constraining our coefficients with $\lambda > 7$ may reduce the variance, and therefore the error, in our predictions. 

In essence, the ridge regression model pushes many of the correlated features toward each other rather than allowing for one to be wildly positive and the other wildly negative.  In addition, many of the less-important features also get pushed toward zero.  This helps to provide clarity in identifying the important signals in our data.

However, ridge regression does not perform feature selection and will retain __all__ available features in the final model.  Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity).  If greater interpretation is necessary and many of the features are redundant or irrelevant then a lasso or elastic net penalty may be preferable.

## Lasso penalty {#lasso}

The lasso (*least absolute shrinkage and selection operator*) penalty [@tibshirani1996regression] is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the $L^2$ norm for an $L^1$ norm: $\lambda \sum^p_{j=1} | \beta_j|$: 

\begin{equation}
\text{minimize } \left( SSE + \lambda \sum^p_{j=1} | \beta_j | \right)
\end{equation}

Whereas the ridge penalty pushes variables to _approximately but not equal to zero_, the lasso penalty will actually push coefficients all the way to zero as illustrated in below.  Switching to the lasso penalty not only improves the model but it also conducts automated feature selection.  

```{r lasso-coef-example, echo=FALSE, fig.cap="Figure: Lasso regression coefficients as $\\lambda$ grows from  $0 \\rightarrow \\infty$.", fig.height=3.5, fig.width=7}
boston_train_x <- model.matrix(cmedv ~ ., pdp::boston)[, -1]
boston_train_y <- pdp::boston$cmedv

# model
boston_lasso <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 1
)

lam <- boston_lasso$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_lasso$a0 %>% names()) %>%
  rename(lambda = ".")

results <- boston_lasso$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)

result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))

ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.05, show.legend = FALSE)
```

In the figure above we see that when $\lambda < 0.01$ all 15 variables are included in the model, when $\lambda \approx 0.5$ 9 variables are retained, and when $log\left(\lambda\right) = 1$ only 5 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal.

## Elastic nets {#elastic}

A generalization of the ridge and lasso penalties, called the _elastic net_ [@zou2005regularization], combines the two penalties:

\begin{equation}
\text{minimize } \left( SSE + \lambda_1 \sum^p_{j=1} \beta_j^2 + \lambda_2 \sum^p_{j=1} | \beta_j | \right)
\end{equation}

Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model.  Furthermore, the process of one being in and one being out is not very systematic.  In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together.  Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. 

```{r elastic-net-coef-example, echo=FALSE, fig.cap="Figure: Elastic net coefficients as $\\lambda$ grows from  $0 \\rightarrow \\infty$.", fig.height=3.5, fig.width=7}
# model
boston_elastic <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = .2
)

lam <- boston_elastic$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_elastic$a0 %>% names()) %>%
  rename(lambda = ".")

results <- boston_elastic$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)

result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))

ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.05, show.legend = FALSE)
```

# Implementation {.tabset}

We will start by applying a simple ridge model but within each language tab we discuss how to apply a simple lasso and elastic net as well. In this example we simplify and focus on only four features: `Gr_Liv_Area`, `Year_Built`, `Garage_Cars`, and `Garage_Area`. In this example we will apply a $\lambda$ penalty parameter equal to 1.

```{block, type="note"}
Since regularized methods apply a penalty to the coefficients, we need to ensure our coefficients are on a common scale. If not, then predictors with naturally larger values (e.g., total square footage) will be penalized more than predictors with naturally smaller values (e.g., total number of rooms).
```


## `r fontawesome::fa("python")` 

The below applies a ridge model but you could just as easily apply a simple lasso and elastic net model with `Lasso()` and `ElasticNet()`. The `alpha` parameter represents the $\lambda$ penalty parameter.

```{block, type='tip'}
`Ridge()`, `Lasso()`, and `ElasticNet()` all provide a `normalize` parameter that you can use to normalize your features. However, the features will be normalized before regression by subtracting the mean and dividing by the l2-norm rather than the mean. If you wish to standardize, we need to use the `StandardScaler` preprocessor before calling fit on an estimator with `normalize=False`.
```


```{python}
# Step 1: get features of interest
features = X_train[["Gr_Liv_Area", "Year_Built", "Garage_Cars", "Garage_Area"]]

# Step 2: standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(features)

# Step 3: create model object
ridge_mod = Ridge(alpha=1)

# Step 4: fit/train model
ridge_fit = ridge_mod.fit(X_train_scaled, y_train)
```

```{python}
ridge_fit.coef_
```


## `r fontawesome::fa("r-project")`

There are a few engines that allow us to apply regularized models. The most popular is glmnet. In R, the $\lambda$ penalty parameter is represented by `penalty` and `mixture` controls the type of penalty (0 = ridge, 1 = lasso, and any value in between represents an elastic net).

```{r}
# Step 1: create ridge model object
ridge_mod <- linear_reg(penalty = 1, mixture = 0) %>%
  set_engine("glmnet")

# Step 2: create model & preprocessing recipe
model_recipe <- recipe(
    Sale_Price ~ Gr_Liv_Area + Year_Built + Garage_Cars + Garage_Area, 
    data = ames_train
  ) %>%
  step_normalize(all_predictors())
  
# Step 3: fit model workflow
ridge_fit <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(ridge_mod) %>%
  fit(data = ames_train)

# Step 4: extract and tidy results
ridge_fit %>%
  pull_workflow_fit() %>%
  tidy()
```

# Tuning

There are two main tuning parameters to consider in regularized models: the strength of the regularization parameter ($\lambda$) and the type of penalty (ridge, lasso, elastic net).

## Tuning regularization strength {.tabset}

First, we'll asses how the regularization strength impacts the performance of our model. In the examples that follow we will use all the features in our Ames housing data; however, we'll stick to the basic preprocessing of standarizing our numeric features and one-hot encoding our categorical features.

### `r fontawesome::fa("python")` 

First, we will standardize and one-hot encode our features.

```{python}
# create new feature set with encoded features
preprocessor = ColumnTransformer(
  remainder="passthrough",
  transformers=[
    ("scale", StandardScaler(), selector(dtype_include="number")),
    ("one-hot", OneHotEncoder(), selector(dtype_include="object"))
  ])

X_train_encoded = preprocessor.fit_transform(X_train)
```

Next, we create a standaard Ridge model object. This same procedure could be done using `Lasso()` or `ElasticNet()`. The last step in this code chunk is to create a tuning grid of penalty parameter values to assess. This example will look at 50 values ranging from 5.0e-06 to 5.0e+09.

```{python}
# create model object
ridge_mod = Ridge()

# define loss function
loss = 'neg_root_mean_squared_error'

# create 5 fold CV object
kfold = KFold(n_splits=5, random_state=123, shuffle=True)

# Create grid of hyperparameter values
hyper_grid = {'alpha': 10**np.linspace(10, -5, 50)*0.5}
```

Now we search for the optimal value using 5-fold cross validation procedure. We see that our optimal regularization strength is:

```{python}
grid_search = GridSearchCV(ridge_mod, hyper_grid, cv=kfold, scoring=loss)
results = grid_search.fit(X_train_encoded, y_train)

# Optimal penalty parameter in grid search
results.best_estimator_
```

Which provides a CV RMSE of the following. With minimal feature engineering this is already the best model performance we've obtained compared to previous modules.

```{python}
# Best model's cross validated RMSE
round(abs(results.best_score_), 2)
```


### `r fontawesome::fa("r-project")`

In R, we'll use `penalty = tune()` when we create our model object. This allows us to create a placeholder for the parameter we want to tune. We also create a tuning grid containing 50 different lambda penalty values to assess. These values range from 1.0e-05 to 1.0+5 (input range is -10 to 5 but these values are log10 transformed).

Our final results are very similar to those found in the Python code chunks. We see that the optimal penalty parameter produces a 5-fold CV RMSE of just under \$31K, which is the best model performance we've obtained compared to previous modules.

```{r}
# create linear model object
ridge_mod <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet")

# create k-fold cross validation object
folds <- vfold_cv(ames_train, v = 5)

# create our model recipe with a tuning option for number of components
model_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# create a hyper parameter tuning grid for penalty
hyper_grid <- grid_regular(penalty(range = c(-10, 5)), levels = 50)

# train our model across the hyper parameter grid
set.seed(123)
results <- tune_grid(ridge_mod, model_recipe, resamples = folds, grid = hyper_grid)

# get best results
show_best(results, metric = "rmse")
```


## Tuning regularization type & strength {.tabset}

Often, the optimal model includes a penalty that is some combination of both $L^1$ and $L^2$ (elastic net), thus we want to tune both the penalty strength ***and*** the type of penalty. The following performs this grid search.

### `r fontawesome::fa("python")` 

In Python we'll use `ElasticNet()`. `ElasticNet()` includes an `l1_ratio` parameter that can be set between 0-1. When `l1_ratio=1` the results are the same as `Ridge()`, when `l1_ratio=0` the results are the same as `Lasso()`, and values in between performs an elastic net with a specified $L^1$ and $L^2$ ratios (i.e. `l1_ratio=0.75` uses a 75% $L^1$ and 25% $L^2$ penalty).

```{block, type="tip"}
You may need to increase the `tol` parameter so the algorithm converges. This is a common concern for lasso models in sci-kit learn. The default `tol`erance level is 0.0001, which I increase in the below model. See [here](https://stackoverflow.com/questions/20681864/lasso-on-sklearn-does-not-converge) for more discussion.
```


```{python}
# create model object
mod = ElasticNet(tol=0.01)

# Create grid of hyperparameter values
hyper_grid = {
  'alpha': 10**np.linspace(10, -5, 10)*0.5,
  'l1_ratio': (0, 0.25, 0.5, 0.75 , 1)
  }
```


```{python}
# 5-fold CV grid search
grid_search = GridSearchCV(mod, hyper_grid, cv=kfold, scoring=loss)
results = grid_search.fit(X_train_encoded, y_train)

# Optimal penalty parameter in grid search
results.best_estimator_
```

```{python}
# Best model's cross validated RMSE
round(abs(results.best_score_), 2)
```


### `r fontawesome::fa("r-project")`

In R we use `tune()` to tune the penalty strength (`penalty`) and the combination (aka `mixture`) of $L^1$ and $L^2$ penalty. We do so by creating a hyper grid that not only includes the penalty strength values but also 5 values of $L^1$ and $L^2$ combinations (0, 0.25, 0.50, 0.75, 1).

```{r}
# create linear model object
mod <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

# create k-fold cross validation object
folds <- vfold_cv(ames_train, v = 5)

# create our model recipe with a tuning option for number of components
model_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# create a hyper parameter tuning grid for penalty
hyper_grid <- grid_regular(
  penalty(range = c(-10, 5)), 
  mixture(), 
  levels = c(mixture = 5, penalty = 50)
  )

# train our model across the hyper parameter grid
set.seed(123)
results <- tune_grid(mod, model_recipe, resamples = folds, grid = hyper_grid)

# get best results
show_best(results)
```


# Feature importance {.tabset}

On thing we have not talked about yet is, once we have a final model how do we know which features are most influential to our model. This concept is known as variable/feature importance. For multiple linear regression models, this is most often measured by the absolute value of the t-statistic for each model parameter used; though simple, the results can be hard to interpret when the model includes interaction effects and complex transformations. 

For regularized models, importance is determined by magnitude of the standardized coefficients. Recall that ridge, lasso, and elastic net models push non-influential features to zero (or near zero). Consequently, very small coefficient values represent features that are not very important while very large coefficient values (whether negative or positive) represent very important features.

## `r fontawesome::fa("python")` 

In Python we need to do a manual process to extract the feature names. We than get the best CV model's coefficients and combine into a DataFrame. For now we'll just focus on the top 25 most influential features in our model.

```{python}
# get feature names
feat_names = ColumnTransformer(
  remainder="passthrough",
  transformers=[
      ("one-hot", OneHotEncoder(), selector(dtype_include="object"))
  ]).fit(X_train).get_feature_names()

# create DataFrame with feature name and coefficients from best model
coef = pd.DataFrame({'feature': feat_names,
                     'coef': results.best_estimator_.coef_})

# indicate if coefficient is positive or negative
coef['abs_coef'] = coef['coef'].abs()
coef['impact'] = np.where(coef['coef']>0, 'positive', 'negative')

# filter for the top 25
top_25_features = coef.nlargest(25, 'abs_coef')
top_25_features.head()
```

We can then take this feature importance DataFrame and plot the coefficient magnitudes. We can see two very influential features, both of which are negative, and then another 6 strongly influential features before tailing off to much smaller magnitudes.

```{python}
# plot feature importance
(ggplot(top_25_features, aes(x='abs_coef', 
                             y='reorder(feature, abs_coef)', 
                             color='impact', 
                             fill='impact'))
 + geom_point()
 + labs(y=None))
```



## `r fontawesome::fa("r-project")`

In R we can use the [vip](https://koalaverse.github.io/vip/index.html) package to simplify the process of extracting feature importance.

```{r}
# identify best model
lowest_rmse <- results %>%
  select_best("rmse")

# extract best model workflow
final_model <- finalize_workflow(
  workflow() %>% add_recipe(model_recipe) %>% add_model(ridge_mod), 
  lowest_rmse)

# extract feature importance for top 25 most influential features
top_25_features <- final_model %>%
  fit(ames_train) %>%
  pull_workflow_fit() %>% 
  vi(lambda = lowest_rmse$penalty) %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  top_n(25, wt = Importance)
```

And finally we can plot the feature importance to identify those features most influential (whether positive or negative) in predicting outcomes.

```{r}
ggplot(top_25_features, aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```



# Final thoughts

Regularized regression provides many great benefits over traditional GLMs when applied to large data sets with lots of features.  It provides a great option for handling the $n > p$ problem, helps minimize the impact of multicollinearity, and can perform automated feature selection. It also has relatively few hyperparameters which makes them easy to tune, computationally efficient compared to other algorithms discussed in later modules, and memory efficient.

However, similar to GLMs, they are not robust to outliers in both the feature and target.  Also, regularized regression models still assume a monotonic linear relationship (always increasing or decreasing in a linear fashion). It is also up to the analyst whether or not to include specific interaction effects.


# Exercises

Using the boston housing dataset:

1. Apply a ridge model with `medv` being the response variable. Perform a cross-validation procedure and tune the model across various penalty parameter values.
   - What is the minimum RMSE?
   - What is the penalty parameter value for the optimal model?
   - What are the coefficients for the optimal model?
   - Plot the top 10 most influential features. Do these features have positive or negative impacts on your response variable?
2. Apply a lasso model with `medv` being the response variable. Perform a cross-validation procedure and tune the model across various penalty parameter values.
   - What is the minimum RMSE?
   - What is the penalty parameter value for the optimal model?
   - What are the coefficients for the optimal model?
   - Plot the top 10 most influential features. Do these features have positive or negative impacts on your response variable?
3. Perform a grid search that assess combinations of penalty type (i.e. ridge, lasso, and elastic net combinations) with penalty parameter magnitude.
   - What is the optimal model's RMSE?
   - What are the parameters (penalty type & magnitude) for the optimal model?
   - How does it compare to your previous models?
   - Plot the top 10 most influential features. Do these features have positive or negative impacts on your response variable?

[üè†](https://github.com/misk-data-science/misk-homl)

# References
